[
  {
    "slug": "llama-4-scout-10m-context",
    "title": "Llama 4 Scout: 10M Token Context on a Single GPU",
    "date": "2026-02-14",
    "tags": [
      "models",
      "meta",
      "open-source"
    ],
    "author": "VibeClaw",
    "image": "/news/llama-4-scout.png",
    "summary": "Meta drops Llama 4 Scout with 10 million token context and 16 mixture-of-experts â€” and it runs on a single H100.",
    "html": "<p>Meta just released <strong>Llama 4 Scout</strong>, and it's a big deal for open-source AI.</p>\n<h2>The Numbers</h2>\n<ul><li><strong>109B parameters</strong> with 16 mixture-of-experts (MoE) â€” only 17B active at any time</li>\n<li><strong>10 million token context window</strong> â€” that's roughly 30 full novels</li>\n<li>Runs on a <strong>single H100 GPU</strong> thanks to the MoE architecture</li>\n<li>Beats Gemma 3, Qwen 2.5, and GPT-4o on most benchmarks</li>\n<li>Licensed under Meta's community license (free for most use)</li>\n</ul>\n<h2>Why It Matters</h2>\n<p>The 10M context window is the headline, but the real story is efficiency. By using mixture-of-experts, Meta gets GPT-4 class performance while only activating 17B parameters per token. That means you can run this on hardware that would choke on a dense 109B model.</p>\n<p>For OpenClaw users, this means you can run a genuinely powerful model locally without renting a GPU cluster.</p>\n<h2>Try It in VibeClaw</h2>\n<p>Boot a sandbox at <a href=\"https://vibeclaw.dev\" target=\"_blank\" rel=\"noopener\">vibeclaw.dev</a> and test Llama models with free API access through OpenRouter. No API key needed.</p>\n<h2>Links</h2>\n<ul><li><a href=\"https://huggingface.co/meta-llama\" target=\"_blank\" rel=\"noopener\">HuggingFace: meta-llama</a></li>\n<li><a href=\"https://ai.meta.com/blog/\" target=\"_blank\" rel=\"noopener\">Meta AI Blog</a></li></ul>"
  },
  {
    "slug": "five-free-models-coding-challenge",
    "title": "We Tested 5 Free Models on the Same Coding Challenge",
    "date": "2026-02-13",
    "tags": [
      "review",
      "comparison",
      "coding",
      "free-models"
    ],
    "author": "VibeClaw",
    "image": "/news/five-free-models.png",
    "summary": "Solar Pro 3, DeepSeek R1, Llama 3.1, Qwen3 8B, and Gemma 3 walk into a coding challenge. Here's what happened.",
    "html": "<p>All five models are free on OpenRouter and available in VibeClaw. We gave them the same task: <strong>build a working todo app with localStorage persistence in vanilla JavaScript</strong>.</p>\n<h2>The Task</h2>\n<blockquote>Write a complete, working todo app in a single HTML file. It should support adding todos, marking them complete, deleting them, and persisting to localStorage. Use vanilla JS, no frameworks. Make it look decent.</blockquote>\n<h2>Results</h2>\n<h3>ðŸ¥‡ DeepSeek R1</h3>\n<strong>Time</strong>: ~45 seconds (long thinking chain)\n<strong>Result</strong>: Perfect. Clean code, good CSS, localStorage working, even added a filter for active/completed. Over-engineered slightly but everything worked first try.\n<h3>ðŸ¥ˆ Solar Pro 3</h3>\n<strong>Time</strong>: ~8 seconds\n<strong>Result</strong>: Excellent. Clean, minimal, everything worked. Slightly less polished CSS than DeepSeek but the code was arguably cleaner. Best speed-to-quality ratio.\n<h3>ðŸ¥‰ Qwen3 8B</h3>\n<strong>Time</strong>: ~12 seconds\n<strong>Result</strong>: Good. Working app, decent styling. Minor issue with the delete button not having hover states. localStorage worked fine.\n<h3>4th â€” Gemma 3 4B</h3>\n<strong>Time</strong>: ~10 seconds\n<strong>Result</strong>: Functional but basic. The CSS was minimal â€” white background, no border-radius, felt like 2015. But the logic was correct.\n<h3>5th â€” Llama 3.1 8B</h3>\n<strong>Time</strong>: ~15 seconds\n<strong>Result</strong>: Mostly worked but had a bug â€” completed todos lost their state on page refresh due to a JSON serialisation issue. The code structure was good but needed a fix.\n<h2>Verdict</h2>\n<strong>For coding tasks</strong>: DeepSeek R1 wins on quality, Solar Pro 3 wins on speed. Both are free.\n<strong>For quick answers</strong>: Solar Pro 3 is the best default â€” fast, accurate, and doesn't burn tokens thinking out loud.\n<strong>The surprise</strong>: Gemma 3 at 4B parameters held its own against 8B models. Google's training data is doing work.\n<h2>Try It Yourself</h2>\n<p>All five models are available for free at <a href=\"https://vibeclaw.dev\" target=\"_blank\" rel=\"noopener\">vibeclaw.dev</a>. Boot a sandbox, pick a flavour, and run your own comparison.</p>"
  },
  {
    "slug": "deepseek-r1-beats-o3",
    "title": "DeepSeek R1: The Open-Source Model That Beats o3",
    "date": "2026-02-12",
    "tags": [
      "models",
      "deepseek",
      "reasoning",
      "open-source"
    ],
    "author": "VibeClaw",
    "image": "/news/deepseek-r1.png",
    "summary": "DeepSeek's R1 model matches or beats OpenAI's o3 on math and coding â€” and it's completely free.",
    "html": "<p>DeepSeek just dropped the <strong>R1 0528 update</strong>, and it's embarrassing some very expensive proprietary models.</p>\n<h2>What Changed</h2>\n<p>The R1 0528 revision brings significant improvements to the chain-of-thought reasoning:</p>\n<ul><li><strong>AIME 2025</strong>: 87.5% (vs o3's 83.3%)</li>\n<li><strong>Codeforces</strong>: 2029 Elo (competitive programmer level)</li>\n<li><strong>GPQA Diamond</strong>: 81.0% (PhD-level science questions)</li>\n<li><strong>LiveCodeBench</strong>: Top 3 across all models tested</li>\n</ul>\nAll of this from a model you can download and run yourself.\n<h2>The Free Model Revolution</h2>\n<p>DeepSeek R1 is available for free on OpenRouter, which means VibeClaw users get access to o3-level reasoning at zero cost. No API key, no subscription, no limits.</p>\n<p>This is what makes open-source AI exciting â€” the gap between \"free\" and \"the best money can buy\" is closing fast.</p>\n<h2>The Catch</h2>\n<p>R1's reasoning chains can be verbose. It thinks out loud â€” sometimes for thousands of tokens before giving you an answer. Great for complex problems, overkill for \"what's the weather.\"</p>\n<h2>Try It</h2>\n<p>R1 is one of the free models available in the <a href=\"https://vibeclaw.dev\" target=\"_blank\" rel=\"noopener\">VibeClaw sandbox</a>. Boot a server and switch to DeepSeek R1 in the flavour dropdown.</p>\n<h2>Links</h2>\n<ul><li><a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" rel=\"noopener\">HuggingFace: deepseek-ai</a></li>\n<li><a href=\"https://arxiv.org/abs/2501.12948\" target=\"_blank\" rel=\"noopener\">DeepSeek R1 Paper</a></li></ul>"
  },
  {
    "slug": "why-browser-ai-agents-matter",
    "title": "Why In-Browser AI Agents Matter",
    "date": "2026-02-11",
    "tags": [
      "think-piece",
      "agents",
      "browser",
      "vibeclaw"
    ],
    "author": "VibeClaw",
    "image": "/news/browser-agents.png",
    "summary": "The next wave of AI isn't in the cloud â€” it's in your browser tab. Here's why that changes everything.",
    "html": "<p>Everyone's building AI agents that run on servers. We think the interesting stuff happens when they run <strong>in your browser</strong>.</p>\n<h2>The Server Problem</h2>\n<p>Running an AI agent today means:</p>\n<p>1. Rent a VPS ($5-50/month)\n2. Install Node.js\n3. Configure API keys\n4. Set up a database\n5. Deal with SSL certificates\n6. Monitor uptime\n7. Pay for it all, every month</p>\n<p>That's a lot of steps before you can even say \"hello\" to your agent.</p>\n<h2>The Browser Solution</h2>\n<p>What if you just... opened a tab?</p>\n<p>VibeClaw runs a complete OpenClaw instance in your browser. The runtime, the virtual filesystem, the agent loop â€” all of it executes in a web worker. Your API calls go directly from your browser to the model provider. Nothing touches our servers.</p>\n<p>This means:</p>\n<ul><li><strong>Zero setup</strong> â€” open the page, click boot, start chatting</li>\n<li><strong>Zero cost to us</strong> â€” we don't proxy your API calls, so we don't pay for your usage</li>\n<li><strong>Zero trust required</strong> â€” your keys never leave your browser</li>\n<li><strong>Instant experimentation</strong> â€” try 10 different agent configs in 10 minutes</li>\n</ul>\n<h2>What You Lose</h2>\n<p>Let's be honest about the tradeoffs:</p>\n<ul><li><strong>No 24/7 operation</strong> â€” close the tab, agent stops</li>\n<li><strong>No persistent memory</strong> â€” (yet â€” we're working on it)</li>\n<li><strong>No incoming messages</strong> â€” can't receive WhatsApp/Telegram while the browser is closed</li>\n<li><strong>Performance ceiling</strong> â€” a browser sandbox isn't as fast as a dedicated server</li>\n</ul>\nFor always-on agents, you still want a server. But for building, testing, experimenting, and learning? The browser wins.\n<h2>The Hybrid Future</h2>\n<p>The interesting play is using VibeClaw to <strong>build</strong> your agent config, test it in the browser sandbox, then <strong>export</strong> it to a real OpenClaw server. Best of both worlds: friction-free experimentation, production-grade deployment.</p>\n<p>That's what we're building toward.</p>\n<h2>Try It</h2>\n<a href=\"https://vibeclaw.dev\" target=\"_blank\" rel=\"noopener\">vibeclaw.dev</a> â€” no sign up, no install, no credit card. Just open and go."
  },
  {
    "slug": "qwen3-235b-hybrid-thinking",
    "title": "Qwen3 235B: Hybrid Thinking Across 119 Languages",
    "date": "2026-02-10",
    "tags": [
      "models",
      "alibaba",
      "multilingual",
      "open-source"
    ],
    "author": "VibeClaw",
    "image": "/news/qwen3-235b.png",
    "summary": "Alibaba's Qwen3 brings a novel hybrid thinking mode â€” toggle between fast responses and deep reasoning on the fly.",
    "html": "<p>Alibaba's <strong>Qwen3 235B</strong> introduces something genuinely new: <strong>hybrid thinking mode</strong>.</p>\n<h2>Fast Mode vs Think Mode</h2>\n<p>Most reasoning models are always-on thinkers. They burn through tokens deliberating even simple questions. Qwen3 lets you toggle:</p>\n<ul><li><strong>Fast mode</strong>: Direct answers, minimal overhead. Like talking to GPT-4.</li>\n<li><strong>Think mode</strong>: Full chain-of-thought reasoning. Like talking to o3.</li>\n</ul>\nYou can switch mid-conversation, or let the model decide based on query complexity.\n<h2>119 Languages</h2>\n<p>Qwen3 isn't just big â€” it's the most multilingual open model available. 119 languages with genuine competence, not just \"technically supports\" quality.</p>\n<p>For teams building agents that serve global users, this matters.</p>\n<h2>The Model Family</h2>\n<table><tr><td>Model</td><td>Parameters</td><td>Active</td><td>License</td></tr>\n</table>\n<table><tr><td>Qwen3 235B</td><td>235B</td><td>22B (MoE)</td><td>Apache 2.0</td></tr>\n<tr><td>Qwen3 32B</td><td>32B</td><td>32B (dense)</td><td>Apache 2.0</td></tr>\n<tr><td>Qwen3 8B</td><td>8B</td><td>8B (dense)</td><td>Apache 2.0</td></tr>\n<tr><td>Qwen3 4B</td><td>4B</td><td>4B (dense)</td><td>Apache 2.0</td></tr>\n</table>\nThe 8B version is free on OpenRouter and available in VibeClaw.\n<h2>Try It</h2>\n<p>The Qwen3 8B model is one of the free options in <a href=\"https://vibeclaw.dev\" target=\"_blank\" rel=\"noopener\">VibeClaw</a>. For the full 235B, you'll need OpenRouter credits or a local setup with serious hardware.</p>\n<h2>Links</h2>\n<ul><li><a href=\"https://huggingface.co/Qwen\" target=\"_blank\" rel=\"noopener\">HuggingFace: Qwen</a></li>\n<li><a href=\"https://qwenlm.github.io/blog/qwen3/\" target=\"_blank\" rel=\"noopener\">Qwen3 Technical Report</a></li></ul>"
  }
]